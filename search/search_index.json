{
    "docs": [
        {
            "location": "/",
            "text": "Docker Scaler\n\u00b6\n\n\nThe goal of the \nDocker Scaler\n project is to provide a REST HTTP interface to scale services and nodes. It sends scaling alerts to \nAlertmanager\n which can be configured to notify you through email, Slack, etc.\n\n\nDocker Scaler\n examples can be found in the \nTutorials\n section located in the left-hand menu.\n\n\nPlease visit the \nConfiguration\n and \nUsage\n sections for details.",
            "title": "Home"
        },
        {
            "location": "/#docker-scaler",
            "text": "The goal of the  Docker Scaler  project is to provide a REST HTTP interface to scale services and nodes. It sends scaling alerts to  Alertmanager  which can be configured to notify you through email, Slack, etc.  Docker Scaler  examples can be found in the  Tutorials  section located in the left-hand menu.  Please visit the  Configuration  and  Usage  sections for details.",
            "title": "Docker Scaler"
        },
        {
            "location": "/service-scale/",
            "text": "Auto-Scaling With Docker Scaler And Instrumented Metrics\n\u00b6\n\n\nDocker Scaler\n provides an alternative to using Jenkins for service scaling shown in Docker Flow Monitor's \nauto-scaling tutorial\n. In this tutorial, we will construct a system that will scale a service based on response time. The following is an overview of the triggered events in our self-adapting system:\n\n\n\n\nThe \ngo-demo\n service response times becomes too high.\n\n\nDocker Flow Monitor\n is querying the services' metrics, notices the high response times, and alerts the \nAlertmanager\n.\n\n\nThe Alertmanager is configured to forward the alert to \nDocker Scaler\n.\n\n\nDocker Scaler\n scales the service up.\n\n\n\n\nThis tutorial assumes you have Docker Machine version v0.8+ that includes Docker Engine v1.12+.\n\n\n\n\nInfo\n\n\nIf you are a Windows user, please run all the examples from \nGit Bash\n (installed through \nDocker for Windows\n). Also, make sure that your Git client is configured to check out the code \nAS-IS\n. Otherwise, Windows might change carriage returns to the Windows format.\n\n\n\n\nSetting Up A Cluster\n\u00b6\n\n\n\n\nInfo\n\n\nFeel free to skip this section if you already have a Swarm cluster that can be used for this tutorial\n\n\n\n\nWe'll create a Swarm cluster consisting of three nodes created with Docker Machine.\n\n\ngit clone https://github.com/thomasjpfan/docker-scaler.git\n\n\ncd\n docker-scaler\n\n./scripts/ds-swarm.sh\n\n\neval\n \n$(\ndocker-machine env swarm-1\n)\n\n\n\n\n\nWe cloned the \nthomasjpfan/docker-scaler\n respository. It contains all the scripts and stack files we will use throughout this tutorial. Next, we executed the \nds-swarm.sh\n script that created the cluster. Finally, we used the \neval\n command to tell our local Docker client to use the remote Docker Engine \nswarm-1\n.\n\n\nDeploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)\n\u00b6\n\n\nFor convenience, we will use \nDocker Flow Proxy\n and \nDocker Flow Swarm Listener\n to get a single access point to the cluster.\n\n\ndocker network create -d overlay proxy\n\ndocker stack deploy \n\\\n\n    -c stacks/docker-flow-proxy-mem.yml \n\\\n\n    proxy\n\n\n\n\nPlease visit \nproxy.dockerflow.com\n and \nswarmlistener.dockerflow.com\n for details on the \nDocker Flow\n stack.\n\n\nDeploying Docker Scaler\n\u00b6\n\n\nWe can now deploy the \nDocker Scaler\n stack:\n\n\ndocker network create -d overlay scaler\n\ndocker stack deploy \n\\\n\n    -c stacks/docker-scaler-basic-tutorial.yml \n\\\n\n    scaler\n\n\n\n\nThis stack defines a single \nDocker Scaler\n service:\n\n\n...\n\n  \nservices\n:\n\n    \nscaler\n:\n\n      \nimage\n:\n \nthomasjpfan/docker-scaler\n\n      \nenvironment\n:\n\n        \n-\n \nALERTMANAGER_ADDRESS=http://alertmanager:9093\n\n      \nvolumes\n:\n\n        \n-\n \n/var/run/docker.sock:/var/run/docker.sock\n\n      \nnetworks\n:\n\n        \n-\n \nscaler\n\n      \ndeploy\n:\n\n        \nplacement\n:\n\n            \nconstraints\n:\n \n[\nnode.role == manager\n]\n\n\n...\n\n\n\n\n\nThis definition constraints \nDocker Scaler\n to run on manager nodes and gives it access to the Docker socket, so that it can scale services in the cluster.\n\n\nDeploying Docker Flow Monitor and Alertmanager\n\u00b6\n\n\nThe next stack defines the \nDocker Flow Monitor\n and \nAlertmanager\n services. Before we deploy the stack, we defined our \nAlertmanager\n configuration as a Docker secret:\n\n\necho\n \n\"global:\n\n\n  slack_api_url: 'https://hooks.slack.com/services/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'\n\n\nroute:\n\n\n  receiver: 'slack'\n\n\n  group_wait: 5s\n\n\n  group_interval: 15s\n\n\n  routes:\n\n\n  - match:\n\n\n      scale: up\n\n\n      type: service\n\n\n    group_by: [service, scale, type]\n\n\n    repeat_interval: 1m\n\n\n    receiver: 'scale'\n\n\n  - match:\n\n\n      scale: down\n\n\n      type: service\n\n\n    group_by: [service, scale, type]\n\n\n    repeat_interval: 4m\n\n\n    receiver: 'scale'\n\n\n  - match_re:\n\n\n      alertname: scale_service|reschedule_service|scale_nodes\n\n\n    group_by: [alertname]\n\n\n    receiver: 'slack-scaler'\n\n\n\nreceivers:\n\n\n  - name: 'slack'\n\n\n    slack_configs:\n\n\n      - send_resolved: true\n\n\n        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'\n\n\n        title_link: 'http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'slack-scaler'\n\n\n    slack_configs:\n\n\n      - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'\n\n\n        color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'\n\n\n        title_link: 'http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts'\n\n\n        text: '{{ .CommonAnnotations.summary }}'\n\n\n  - name: 'scale'\n\n\n    webhook_configs:\n\n\n      - send_resolved: false\n\n\n        url: 'http://scaler:8080/v1/scale-service'\n\n\n\"\n \n|\n docker secret create alert_manager_config -\n\n\nThis configuration groups alerts by their \nservice\n, \nscale\n, and \ntype\n labels. The \nroutes\n section defines a \nmatch_re\n entry, that directs scale alerts to the \nscale\n reciever. Another route is configured to direct alerts from the \nscaler\n service to the \nslack-scaler\n receiver. The \nrepeat_interval\n is set to one minute for the \nscale up\n route and four minutes for the \nscale down\n route. This allows for the service to scale up quickly when the response time is high and scale down slowly when the response time drops back down.\n\n\ndocker network create -d overlay monitor\n\n\nDOMAIN\n=\n$(\ndocker-machine ip swarm-1\n)\n \n\\\n\n    docker stack deploy \n\\\n\n    -c stacks/docker-flow-monitor-slack.yml \n\\\n\n    monitor\n\n\n\n\nThe \nalert-manager\n service is configured to read the \nalert_manager_config\n secret in the stack definition as follows:\n\n\n...\n\n  \nalert-manager\n:\n\n    \nimage\n:\n \nprom/alertmanager\n\n    \nnetworks\n:\n\n      \n-\n \nmonitor\n\n      \n-\n \nscaler\n\n    \nsecrets\n:\n\n      \n-\n \nalert_manager_config\n\n    \ncommand\n:\n \n-config.file=/run/secrets/alert_manager_config -storage.path=/alertmanager\n\n\n...\n\n\n\n\n\nWith access to the \nscaler\n network, \nalert-manager\n can send scaling requests to the \nscaler\n service. For information about the Docker Flow Monitor stack can be found in its \ndocumentation\n.\n\n\nLet us confirm that the \nmonitor\n stack is up and running:\n\n\ndocker stack ps monitor\n\n\n\n\nPlease wait a few moments for all the replicas to have the status \nrunning\n. After the \nmonitor\n stack is up and running, we can start deploying the \ngo-demo_main\n service!\n\n\nDeploying Instrumented Service\n\u00b6\n\n\nThe \ngo-demo\n service already exposes response time metrics with labels for \nDocker Flow Monitor\n to scrape. We can deploy the service to be scaled based on the response time metrics:\n\n\ndocker stack deploy \n\\\n\n    -c stacks/go-demo-instrument-alert-short.yml \n\\\n\n    go-demo\n\n\n\n\nThe full stack definition can be found at \ngo-demo-instrument-alert-short.yml\n. We will focus on the service labels for the \ngo-demo_main\n service relating to scaling and alerting:\n\n\nmain\n:\n\n  \n...\n\n  \ndeploy\n:\n\n    \n...\n\n    \nlabels\n:\n\n      \n...\n\n      \n- com.df.scaleMin=2\n\n      \n- com.df.scaleMax=7\n\n      \n- com.df.scaleDownBy=1\n\n      \n- com.df.scaleUpBy=2\n\n      \n- com.df.scrapePort=8080\n\n      \n- com.df.alertName.1=resptimeabove\n\n      \n- com.df.alertIf.1=@resp_time_above:0.1,5m,0.99\n\n      \n- com.df.alertName.2=resptimebelow\n\n      \n- com.df.alertIf.2=@resp_time_below:0.025,5m,0.75\n\n    \n...\n\n\n\nThe \nscaleMin\n and \nscaleMax\n labels are used by \nDocker Scaler\n to bound the number replicas for the \ngo-main_main\n service. The \nalertName\n, \nalertIf\n and \nalertFor\n labels uses the \nAlertIf Parameter Shortcuts\n for creating full Prometheus expressions that translate into alerts. We can view the alerts generated by these labels:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nDocker Flow Monitor translates the alert labeled \nresp_time_above\n into an alert called \ngodemo_main_resp_time_above\n with the following definition:\n\n\nALERT godemo_main_resp_tim_eabove\n  IF sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.1\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) < 0.99\n  LABELS {receiver=\"system\", scale=\"up\", service=\"go-demo_main\"}\n  ANNOTATIONS {summary=\"Response time of the service go-demo_main is above 0.1\"}\n\n\n\n\nThis alert is triggered when the response times of the \n0.1\n seconds bucket is above 99% for over five minutes. Notice that the alert is labeled with \nscale=up\n to comminucate to the \nAlertmanager\n that the \ngo-demo_main\n service should be scaled up.\n\n\nSimiliarly, the alert labeled \nresp_time_below\n is translated into an alert called \ngodemo_main_resp_time_below\n. It is labeled with \nscale=down\n to trigger a de-scaling event:\n\n\nALERT godemo_main_resp_time_below\n  IF sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.025\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) > 0.75\n  LABELS {receiver=\"system\", scale=\"down\", service=\"go-demo_main\"}\n  ANNOTATIONS {summary=\"Response time of the service go-demo_main is below 0.025\"}\n\n\n\n\nLet's confirm that the go-demo stack is up-and-running:\n\n\ndocker stack ps -f desired-state\n=\nrunning go-demo\n\n\n\n\nThere should be three replicas of the \ngo-demo_main\n service and one replica of the \ngo-demo_db\n service. Please wait for all replicas to be up and running.\n\n\nWe can confirm that \nDocker Flow Monitor\n is monitoring the \ngo-demo\n replicas:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/targets\"\n\n\n\n\n\nThere should be two or three targets depending on whether Prometheus already sent the alert to de-scale the service.\n\n\nAutomatically Scaling Services\n\u00b6\n\n\nLet's go back to the Prometheus' alert screen:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nBy this time, the \ngodemo_main_resp_time_below\n alert should be red since the \ngo-demo_main\n service has a response faster than twenty-five milliseconds limit we set. The Alertmanager recieves this alert and sends a \nPOST\n request to the \ndocker-scaler\n service to scale down \ngo-demo\n. The label \ncom.df.scaleDownBy\n on \ngo-demo_main\n is set to 1 thus the number of replicas goes from 4 to 3.\n\n\nLet's look at the logs of \ndocker-scaler\n:\n\n\ndocker service logs scaler_scaler\n\n\n\n\nThere should be a log message that states \nScaling go-demo_main from 4 to 3 replicas (min: 2)\n. We can check that this happened:\n\n\ndocker service ls -f \nname\n=\ngo-demo_main\n\n\n\n\nThe output should be similar to the following:\n\n\nNAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          3/3                 vfarcic/go-demo:latest\n\n\n\n\nPlease visit the \n#df-monitor-tests\n channel inside \ndevops20.slack.com\n and you should see a Slack notification. If this is your first visit to \ndevops20\n on Slack, you'll have to register through \nslack.devops20toolkit.com\n.\n\n\nLet's see what happens when response times of the service becomes too high by sending requests that will result in high response times:\n\n\nfor\n i in \n{\n1\n..30\n}\n;\n \ndo\n\n    \nDELAY\n=\n$\n[\n \n$RANDOM\n % \n6000\n \n]\n\n    curl \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/demo/hello?delay=\n$DELAY\n\"\n\n\ndone\n\n\n\n\n\nLet's look at the alerts:\n\n\nopen \n\"http://\n$(\ndocker-machine ip swarm-1\n)\n/monitor/alerts\"\n\n\n\n\n\nThe \ngodemo_main_resp_time_above\n turned red indicating that the threshold is reached. \nAlertmanager\n receives the alert, sends a \nPOST\n request to the \ndocker-scaler\n service, and \ndocker-scaler\n scales \ngo-demo_main\n up by the value of \ncom.df.scaleUpBy\n. In this case, the value of \ncom.df.scaleUpBy\n is two. Let's look at the logs of \ndocker-scaler\n:\n\n\ndocker service logs scaler_docker-scaler\n\n\n\n\nThere should be a log message that states \nScaling go-demo_main from 3 to 5 replicas (max: 7)\n. This message is also sent through Slack to notify us of this scaling event.\n\n\nWe can confirm that the number of replicas indeed scaled to three by querying the stack processes:\n\n\ndocker service ls -f \nname\n=\ngo-demo_main\n\n\n\n\nThe output should look similar to the following:\n\n\nNAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          5/5                 vfarcic/go-demo:latest\n\n\n\n\nWhat Now?\n\u00b6\n\n\nYou saw a simple example of a system that automatically scales and de-scales services. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.\n\n\nPlease remove the demo cluster we created and free your resources:\n\n\ndocker-machine rm -f swarm-1 swarm-2 swarm-3",
            "title": "Service Scaling"
        },
        {
            "location": "/service-scale/#auto-scaling-with-docker-scaler-and-instrumented-metrics",
            "text": "Docker Scaler  provides an alternative to using Jenkins for service scaling shown in Docker Flow Monitor's  auto-scaling tutorial . In this tutorial, we will construct a system that will scale a service based on response time. The following is an overview of the triggered events in our self-adapting system:   The  go-demo  service response times becomes too high.  Docker Flow Monitor  is querying the services' metrics, notices the high response times, and alerts the  Alertmanager .  The Alertmanager is configured to forward the alert to  Docker Scaler .  Docker Scaler  scales the service up.   This tutorial assumes you have Docker Machine version v0.8+ that includes Docker Engine v1.12+.   Info  If you are a Windows user, please run all the examples from  Git Bash  (installed through  Docker for Windows ). Also, make sure that your Git client is configured to check out the code  AS-IS . Otherwise, Windows might change carriage returns to the Windows format.",
            "title": "Auto-Scaling With Docker Scaler And Instrumented Metrics"
        },
        {
            "location": "/service-scale/#setting-up-a-cluster",
            "text": "Info  Feel free to skip this section if you already have a Swarm cluster that can be used for this tutorial   We'll create a Swarm cluster consisting of three nodes created with Docker Machine.  git clone https://github.com/thomasjpfan/docker-scaler.git cd  docker-scaler\n\n./scripts/ds-swarm.sh eval   $( docker-machine env swarm-1 )   We cloned the  thomasjpfan/docker-scaler  respository. It contains all the scripts and stack files we will use throughout this tutorial. Next, we executed the  ds-swarm.sh  script that created the cluster. Finally, we used the  eval  command to tell our local Docker client to use the remote Docker Engine  swarm-1 .",
            "title": "Setting Up A Cluster"
        },
        {
            "location": "/service-scale/#deploying-docker-flow-proxy-dfp-and-docker-flow-swarm-listener-dfsl",
            "text": "For convenience, we will use  Docker Flow Proxy  and  Docker Flow Swarm Listener  to get a single access point to the cluster.  docker network create -d overlay proxy\n\ndocker stack deploy  \\ \n    -c stacks/docker-flow-proxy-mem.yml  \\ \n    proxy  Please visit  proxy.dockerflow.com  and  swarmlistener.dockerflow.com  for details on the  Docker Flow  stack.",
            "title": "Deploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)"
        },
        {
            "location": "/service-scale/#deploying-docker-scaler",
            "text": "We can now deploy the  Docker Scaler  stack:  docker network create -d overlay scaler\n\ndocker stack deploy  \\ \n    -c stacks/docker-scaler-basic-tutorial.yml  \\ \n    scaler  This stack defines a single  Docker Scaler  service:  ... \n   services : \n     scaler : \n       image :   thomasjpfan/docker-scaler \n       environment : \n         -   ALERTMANAGER_ADDRESS=http://alertmanager:9093 \n       volumes : \n         -   /var/run/docker.sock:/var/run/docker.sock \n       networks : \n         -   scaler \n       deploy : \n         placement : \n             constraints :   [ node.role == manager ]  ...   This definition constraints  Docker Scaler  to run on manager nodes and gives it access to the Docker socket, so that it can scale services in the cluster.",
            "title": "Deploying Docker Scaler"
        },
        {
            "location": "/service-scale/#deploying-docker-flow-monitor-and-alertmanager",
            "text": "The next stack defines the  Docker Flow Monitor  and  Alertmanager  services. Before we deploy the stack, we defined our  Alertmanager  configuration as a Docker secret:  echo   \"global:    slack_api_url: 'https://hooks.slack.com/services/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'  route:    receiver: 'slack'    group_wait: 5s    group_interval: 15s    routes:    - match:        scale: up        type: service      group_by: [service, scale, type]      repeat_interval: 1m      receiver: 'scale'    - match:        scale: down        type: service      group_by: [service, scale, type]      repeat_interval: 4m      receiver: 'scale'    - match_re:        alertname: scale_service|reschedule_service|scale_nodes      group_by: [alertname]      receiver: 'slack-scaler'  receivers:    - name: 'slack'      slack_configs:        - send_resolved: true          title: '[{{ .Status | toUpper }}] {{ .GroupLabels.service }} service is in danger!'          title_link: 'http:// $( docker-machine ip swarm-1 ) /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'slack-scaler'      slack_configs:        - title: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.request }}'          color: '{{ if eq .CommonLabels.status \\\"error\\\" }}danger{{ else }}good{{ end }}'          title_link: 'http:// $( docker-machine ip swarm-1 ) /monitor/alerts'          text: '{{ .CommonAnnotations.summary }}'    - name: 'scale'      webhook_configs:        - send_resolved: false          url: 'http://scaler:8080/v1/scale-service'  \"   |  docker secret create alert_manager_config - \nThis configuration groups alerts by their  service ,  scale , and  type  labels. The  routes  section defines a  match_re  entry, that directs scale alerts to the  scale  reciever. Another route is configured to direct alerts from the  scaler  service to the  slack-scaler  receiver. The  repeat_interval  is set to one minute for the  scale up  route and four minutes for the  scale down  route. This allows for the service to scale up quickly when the response time is high and scale down slowly when the response time drops back down.  docker network create -d overlay monitor DOMAIN = $( docker-machine ip swarm-1 )   \\ \n    docker stack deploy  \\ \n    -c stacks/docker-flow-monitor-slack.yml  \\ \n    monitor  The  alert-manager  service is configured to read the  alert_manager_config  secret in the stack definition as follows:  ... \n   alert-manager : \n     image :   prom/alertmanager \n     networks : \n       -   monitor \n       -   scaler \n     secrets : \n       -   alert_manager_config \n     command :   -config.file=/run/secrets/alert_manager_config -storage.path=/alertmanager  ...   With access to the  scaler  network,  alert-manager  can send scaling requests to the  scaler  service. For information about the Docker Flow Monitor stack can be found in its  documentation .  Let us confirm that the  monitor  stack is up and running:  docker stack ps monitor  Please wait a few moments for all the replicas to have the status  running . After the  monitor  stack is up and running, we can start deploying the  go-demo_main  service!",
            "title": "Deploying Docker Flow Monitor and Alertmanager"
        },
        {
            "location": "/service-scale/#deploying-instrumented-service",
            "text": "The  go-demo  service already exposes response time metrics with labels for  Docker Flow Monitor  to scrape. We can deploy the service to be scaled based on the response time metrics:  docker stack deploy  \\ \n    -c stacks/go-demo-instrument-alert-short.yml  \\ \n    go-demo  The full stack definition can be found at  go-demo-instrument-alert-short.yml . We will focus on the service labels for the  go-demo_main  service relating to scaling and alerting:  main : \n   ... \n   deploy : \n     ... \n     labels : \n       ... \n       - com.df.scaleMin=2 \n       - com.df.scaleMax=7 \n       - com.df.scaleDownBy=1 \n       - com.df.scaleUpBy=2 \n       - com.df.scrapePort=8080 \n       - com.df.alertName.1=resptimeabove \n       - com.df.alertIf.1=@resp_time_above:0.1,5m,0.99 \n       - com.df.alertName.2=resptimebelow \n       - com.df.alertIf.2=@resp_time_below:0.025,5m,0.75 \n     ...  \nThe  scaleMin  and  scaleMax  labels are used by  Docker Scaler  to bound the number replicas for the  go-main_main  service. The  alertName ,  alertIf  and  alertFor  labels uses the  AlertIf Parameter Shortcuts  for creating full Prometheus expressions that translate into alerts. We can view the alerts generated by these labels:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   Docker Flow Monitor translates the alert labeled  resp_time_above  into an alert called  godemo_main_resp_time_above  with the following definition:  ALERT godemo_main_resp_tim_eabove\n  IF sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.1\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) < 0.99\n  LABELS {receiver=\"system\", scale=\"up\", service=\"go-demo_main\"}\n  ANNOTATIONS {summary=\"Response time of the service go-demo_main is above 0.1\"}  This alert is triggered when the response times of the  0.1  seconds bucket is above 99% for over five minutes. Notice that the alert is labeled with  scale=up  to comminucate to the  Alertmanager  that the  go-demo_main  service should be scaled up.  Similiarly, the alert labeled  resp_time_below  is translated into an alert called  godemo_main_resp_time_below . It is labeled with  scale=down  to trigger a de-scaling event:  ALERT godemo_main_resp_time_below\n  IF sum(rate(http_server_resp_time_bucket{job=\"go-demo_main\",le=\"0.025\"}[5m])) / sum(rate(http_server_resp_time_count{job=\"go-demo_main\"}[5m])) > 0.75\n  LABELS {receiver=\"system\", scale=\"down\", service=\"go-demo_main\"}\n  ANNOTATIONS {summary=\"Response time of the service go-demo_main is below 0.025\"}  Let's confirm that the go-demo stack is up-and-running:  docker stack ps -f desired-state = running go-demo  There should be three replicas of the  go-demo_main  service and one replica of the  go-demo_db  service. Please wait for all replicas to be up and running.  We can confirm that  Docker Flow Monitor  is monitoring the  go-demo  replicas:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/targets\"   There should be two or three targets depending on whether Prometheus already sent the alert to de-scale the service.",
            "title": "Deploying Instrumented Service"
        },
        {
            "location": "/service-scale/#automatically-scaling-services",
            "text": "Let's go back to the Prometheus' alert screen:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   By this time, the  godemo_main_resp_time_below  alert should be red since the  go-demo_main  service has a response faster than twenty-five milliseconds limit we set. The Alertmanager recieves this alert and sends a  POST  request to the  docker-scaler  service to scale down  go-demo . The label  com.df.scaleDownBy  on  go-demo_main  is set to 1 thus the number of replicas goes from 4 to 3.  Let's look at the logs of  docker-scaler :  docker service logs scaler_scaler  There should be a log message that states  Scaling go-demo_main from 4 to 3 replicas (min: 2) . We can check that this happened:  docker service ls -f  name = go-demo_main  The output should be similar to the following:  NAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          3/3                 vfarcic/go-demo:latest  Please visit the  #df-monitor-tests  channel inside  devops20.slack.com  and you should see a Slack notification. If this is your first visit to  devops20  on Slack, you'll have to register through  slack.devops20toolkit.com .  Let's see what happens when response times of the service becomes too high by sending requests that will result in high response times:  for  i in  { 1 ..30 } ;   do \n     DELAY = $ [   $RANDOM  %  6000   ] \n    curl  \"http:// $( docker-machine ip swarm-1 ) /demo/hello?delay= $DELAY \"  done   Let's look at the alerts:  open  \"http:// $( docker-machine ip swarm-1 ) /monitor/alerts\"   The  godemo_main_resp_time_above  turned red indicating that the threshold is reached.  Alertmanager  receives the alert, sends a  POST  request to the  docker-scaler  service, and  docker-scaler  scales  go-demo_main  up by the value of  com.df.scaleUpBy . In this case, the value of  com.df.scaleUpBy  is two. Let's look at the logs of  docker-scaler :  docker service logs scaler_docker-scaler  There should be a log message that states  Scaling go-demo_main from 3 to 5 replicas (max: 7) . This message is also sent through Slack to notify us of this scaling event.  We can confirm that the number of replicas indeed scaled to three by querying the stack processes:  docker service ls -f  name = go-demo_main  The output should look similar to the following:  NAME                MODE                REPLICAS            IMAGE                    PORTS\ngo-demo_main        replicated          5/5                 vfarcic/go-demo:latest",
            "title": "Automatically Scaling Services"
        },
        {
            "location": "/service-scale/#what-now",
            "text": "You saw a simple example of a system that automatically scales and de-scales services. Feel free to add additional metrics and services to this self-adapting system to customize it to your needs.  Please remove the demo cluster we created and free your resources:  docker-machine rm -f swarm-1 swarm-2 swarm-3",
            "title": "What Now?"
        },
        {
            "location": "/aws-node-scale/",
            "text": "AWS Node Scaling\n\u00b6\n\n\nWork in progress.\n\n\nSetting Up An AWS Cluster\n\u00b6\n\n\nDeploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)\n\u00b6\n\n\nDeploying Docker Flow Monitor and Alertmanager\n\u00b6\n\n\nDeploying Instrumented Service\n\u00b6\n\n\nAutomatically Scaling Nodes\n\u00b6\n\n\nWhat Now?\n\u00b6",
            "title": "AWS Node Scaling"
        },
        {
            "location": "/aws-node-scale/#aws-node-scaling",
            "text": "Work in progress.",
            "title": "AWS Node Scaling"
        },
        {
            "location": "/aws-node-scale/#setting-up-an-aws-cluster",
            "text": "",
            "title": "Setting Up An AWS Cluster"
        },
        {
            "location": "/aws-node-scale/#deploying-docker-flow-proxy-dfp-and-docker-flow-swarm-listener-dfsl",
            "text": "",
            "title": "Deploying Docker Flow Proxy (DFP) and Docker Flow Swarm Listener (DFSL)"
        },
        {
            "location": "/aws-node-scale/#deploying-docker-flow-monitor-and-alertmanager",
            "text": "",
            "title": "Deploying Docker Flow Monitor and Alertmanager"
        },
        {
            "location": "/aws-node-scale/#deploying-instrumented-service",
            "text": "",
            "title": "Deploying Instrumented Service"
        },
        {
            "location": "/aws-node-scale/#automatically-scaling-nodes",
            "text": "",
            "title": "Automatically Scaling Nodes"
        },
        {
            "location": "/aws-node-scale/#what-now",
            "text": "",
            "title": "What Now?"
        },
        {
            "location": "/usage/",
            "text": "Usage\n\u00b6\n\n\nDocker Scaler\n is controlled by sending HTTP requests to \n[SCALER_IP]:[SCALER_PORT]\n\n\nScaling Services\n\u00b6\n\n\nThis request queries docker service labels: \ncom.df.scaleMin\n, \ncom.df.scaleMax\n, \ncom.df.scaleDownBy\n, \ncom.df.scaleUpBy\n to determine how much to scale the service. Please see \nconfiguration\n for details.\n\n\n\n\n\n\nURL:\n\n    \n/v1/scale-service\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nRequest Body:\n\n\n\n\n\n\n{\n\n    \n\"groupLabels\"\n:\n \n{\n\n        \n\"scale\"\n:\n \n\"up\"\n,\n\n        \n\"service\"\n:\n \n\"example_web\"\n\n    \n}\n\n\n}\n\n\n\n\n\nThe \nservice\n value is the name of the service to scale. The \nscale\n value accepts \nup\n for scaling up and \ndown\n for scaling down.\n\n\nRescheduling All Services\n\u00b6\n\n\nThis request only reschedule services with label: \ncom.df.reschedule=true\n. See \nConfiguration\n to change this default.\n\n\n\n\n\n\nURL:\n\n    \n/v1/reschedule-services\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nRescheduling One Service\n\u00b6\n\n\nThis request only reschedule target service with label: \ncom.df.reschedule=true\n. See \nConfiguration\n to change this default.\n\n\n\n\n\n\nURL:\n\n    \n/v1/reschedule-service\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nQuery Parameters:\n\n\n\n\n\n\n\n\n\n\n\n\nQuery\n\n\nDescription\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nservice\n\n\nName of service to reschedule\n\n\nyes\n\n\n\n\n\n\n\n\nScaling Nodes\n\u00b6\n\n\n\n\n\n\nURL:\n\n    \n/v1/scale-nodes\n\n\n\n\n\n\nMethod:\n\n    \nPOST\n\n\n\n\n\n\nQuery Parameters:\n\n\n\n\n\n\n\n\n\n\n\n\nQuery\n\n\nDescription\n\n\nRequired\n\n\n\n\n\n\n\n\n\n\nby\n\n\nThe number of nodes to scale up or down by\n\n\nyes\n\n\n\n\n\n\n\n\n\n\nRequest Body:\n\n\n\n\n{\n\n    \n\"groupLabels\"\n:\n \n{\n\n        \n\"scale\"\n:\n \n\"down\"\n,\n\n    \n}\n\n\n}\n\n\n\n\n\nThe \nscale\n value accepts \nup\n for scaling up and \ndown\n for scaling down.",
            "title": "Usage"
        },
        {
            "location": "/usage/#usage",
            "text": "Docker Scaler  is controlled by sending HTTP requests to  [SCALER_IP]:[SCALER_PORT]",
            "title": "Usage"
        },
        {
            "location": "/usage/#scaling-services",
            "text": "This request queries docker service labels:  com.df.scaleMin ,  com.df.scaleMax ,  com.df.scaleDownBy ,  com.df.scaleUpBy  to determine how much to scale the service. Please see  configuration  for details.    URL: \n     /v1/scale-service    Method: \n     POST    Request Body:    { \n     \"groupLabels\" :   { \n         \"scale\" :   \"up\" , \n         \"service\" :   \"example_web\" \n     }  }   The  service  value is the name of the service to scale. The  scale  value accepts  up  for scaling up and  down  for scaling down.",
            "title": "Scaling Services"
        },
        {
            "location": "/usage/#rescheduling-all-services",
            "text": "This request only reschedule services with label:  com.df.reschedule=true . See  Configuration  to change this default.    URL: \n     /v1/reschedule-services    Method: \n     POST",
            "title": "Rescheduling All Services"
        },
        {
            "location": "/usage/#rescheduling-one-service",
            "text": "This request only reschedule target service with label:  com.df.reschedule=true . See  Configuration  to change this default.    URL: \n     /v1/reschedule-service    Method: \n     POST    Query Parameters:       Query  Description  Required      service  Name of service to reschedule  yes",
            "title": "Rescheduling One Service"
        },
        {
            "location": "/usage/#scaling-nodes",
            "text": "URL: \n     /v1/scale-nodes    Method: \n     POST    Query Parameters:       Query  Description  Required      by  The number of nodes to scale up or down by  yes      Request Body:   { \n     \"groupLabels\" :   { \n         \"scale\" :   \"down\" , \n     }  }   The  scale  value accepts  up  for scaling up and  down  for scaling down.",
            "title": "Scaling Nodes"
        },
        {
            "location": "/configuration/",
            "text": "Configuring Docker Scaler\n\u00b6\n\n\nDocker Scaler\n can be configured through Docker enivonment variables and/or by creating a new image based on \nthomasjpfan/docker-scaler\n\n\nService Scaling Environment Variables\n\u00b6\n\n\n\n\nTip\n\n\nThe \nDocker Scaler\n container can be configured through envionment variables\n\n\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to service scaling.\n\n\n\n\n\n\n\n\nVariable\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMIN_SCALE_LABEL\n\n\nService label key with value representing the minimum number of replicas.\nDefault:\n \ncom.df.scaleMin\n\n\n\n\n\n\nMAX_SCALE_LABEL\n\n\nService label key with value representing the maximum number of replicas.\nDefault:\n \ncom.df.scaleMax\n\n\n\n\n\n\nSCALE_DOWN_BY_LABEL\n\n\nService label key with value representing the number of replicas to scale down by.\nDefault:\n \ncom.df.scaleDownBy\n\n\n\n\n\n\nSCALE_UP_BY_LABEL\n\n\nService label key with value representing the number of replicas to scale up by.\nDefault:\n \ncom.df.scaleUpBy\n\n\n\n\n\n\nDEFAULT_MIN_REPLICAS\n\n\nDefault minimum number of replicas for a service.\nDefault:\n 1\n\n\n\n\n\n\nDEFAULT_MAX_REPLICAS\n\n\nDefault maximum number of replicas for a service.\nDefault:\n 5\n\n\n\n\n\n\nDEFAULT_SCALE_SERVICE_DOWN_BY\n\n\nDefault number of replicas to scale service down by.\nDefault:\n 1\n\n\n\n\n\n\nDEFAULT_SCALE_SERVICE_UP_BY\n\n\nDefault number of replicas to scale service up by.\nDefault:\n 1\n\n\n\n\n\n\nALERTMANAGER_ADDRESS\n\n\nAddress for alertmanager.\nDefault:\n \nhttp://alertmanager:9093\n\n\n\n\n\n\nALERT_TIMEOUT\n\n\nAlert timeout duration (seconds).\nDefault:\n 10\n\n\n\n\n\n\nRESCHEDULE_FILTER_LABEL\n\n\nServices with this label will be rescheduled after node scaling.\nDefault:\n \ncom.df.reschedule=true\"\n\n\n\n\n\n\nRESCHEDULE_TICKER_INTERVAL\n\n\nDuration to wait when checking for nodes to come up (seconds).\nDefault:\n 20\n\n\n\n\n\n\nRESCHEDULE_TIMEOUT\n\n\nTime to wait for nodes to come up during rescheduling (seconds).\nDefault:\n 1000\n\n\n\n\n\n\nRESCHEDULE_ENV_KEY\n\n\nKey for env variable when rescheduling services.\nDefault:\n \nRESCHEDULE_DATE\n\n\n\n\n\n\n\n\nNode Scaling Environment Variables\n\u00b6\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to node scaling.\n\n\n| NODE_SCALER_BACKEND | Backend of node backend.\nAccepted Values:\n [none, aws]\nDefault:\n none |\n| DEFAULT_MIN_MANAGER_NODES | Miniumum number of manager nodes.\nDefault:\n 3 |\n| DEFAULT_MAX_MANAGER_NODES | Maximum number of manager nodes.\nDefault:\n 7 |\n| DEFAULT_MIN_WORKER_NODES | Miniumum number of worker nodes.\nDefault:\n 1 |\n| DEFAULT_MAX_WORKER_NODES | Maximum number of worker nodes.\nDefault:\n 5 |\n\n\nAWS Node Scaling Envronment Variables\n\u00b6\n\n\nThe following environment variables can be used to configure the \nDocker Scaler\n relating to AWS node scaling.\n\n\n| AWS_ENV_FILE | Location of AWS env file used when \nNODE_SCALER_BACKEND\n is sent to \naws\n.\nDefault:\n \n/run/secrets/aws\n |\n| AWS_DEFAULT_REGION | Default AWS region.\nDefault:\n \nus-east-1\n |\n| AWS_MANAGER_GROUP_NAME | AWS group name for manager nodes. |\n| AWS_WORKER_GROUP_NAME | AWS group name for worker nodes.\n\n\nAWS Secrets file\n\u00b6\n\n\nAWS secret file defines the necessary environment variables to authenticate with AWS.\n\n\necho\n \n'export AWS_ACCESS_KEY_ID=xxxx\n\n\nexport AWS_SECRET_ACCESS_KEY=xxxx\n\n\n'\n \n|\n docker secret create aws -",
            "title": "Configuration"
        },
        {
            "location": "/configuration/#configuring-docker-scaler",
            "text": "Docker Scaler  can be configured through Docker enivonment variables and/or by creating a new image based on  thomasjpfan/docker-scaler",
            "title": "Configuring Docker Scaler"
        },
        {
            "location": "/configuration/#service-scaling-environment-variables",
            "text": "Tip  The  Docker Scaler  container can be configured through envionment variables   The following environment variables can be used to configure the  Docker Scaler  relating to service scaling.     Variable  Description      MIN_SCALE_LABEL  Service label key with value representing the minimum number of replicas. Default:   com.df.scaleMin    MAX_SCALE_LABEL  Service label key with value representing the maximum number of replicas. Default:   com.df.scaleMax    SCALE_DOWN_BY_LABEL  Service label key with value representing the number of replicas to scale down by. Default:   com.df.scaleDownBy    SCALE_UP_BY_LABEL  Service label key with value representing the number of replicas to scale up by. Default:   com.df.scaleUpBy    DEFAULT_MIN_REPLICAS  Default minimum number of replicas for a service. Default:  1    DEFAULT_MAX_REPLICAS  Default maximum number of replicas for a service. Default:  5    DEFAULT_SCALE_SERVICE_DOWN_BY  Default number of replicas to scale service down by. Default:  1    DEFAULT_SCALE_SERVICE_UP_BY  Default number of replicas to scale service up by. Default:  1    ALERTMANAGER_ADDRESS  Address for alertmanager. Default:   http://alertmanager:9093    ALERT_TIMEOUT  Alert timeout duration (seconds). Default:  10    RESCHEDULE_FILTER_LABEL  Services with this label will be rescheduled after node scaling. Default:   com.df.reschedule=true\"    RESCHEDULE_TICKER_INTERVAL  Duration to wait when checking for nodes to come up (seconds). Default:  20    RESCHEDULE_TIMEOUT  Time to wait for nodes to come up during rescheduling (seconds). Default:  1000    RESCHEDULE_ENV_KEY  Key for env variable when rescheduling services. Default:   RESCHEDULE_DATE",
            "title": "Service Scaling Environment Variables"
        },
        {
            "location": "/configuration/#node-scaling-environment-variables",
            "text": "The following environment variables can be used to configure the  Docker Scaler  relating to node scaling.  | NODE_SCALER_BACKEND | Backend of node backend. Accepted Values:  [none, aws] Default:  none |\n| DEFAULT_MIN_MANAGER_NODES | Miniumum number of manager nodes. Default:  3 |\n| DEFAULT_MAX_MANAGER_NODES | Maximum number of manager nodes. Default:  7 |\n| DEFAULT_MIN_WORKER_NODES | Miniumum number of worker nodes. Default:  1 |\n| DEFAULT_MAX_WORKER_NODES | Maximum number of worker nodes. Default:  5 |",
            "title": "Node Scaling Environment Variables"
        },
        {
            "location": "/configuration/#aws-node-scaling-envronment-variables",
            "text": "The following environment variables can be used to configure the  Docker Scaler  relating to AWS node scaling.  | AWS_ENV_FILE | Location of AWS env file used when  NODE_SCALER_BACKEND  is sent to  aws . Default:   /run/secrets/aws  |\n| AWS_DEFAULT_REGION | Default AWS region. Default:   us-east-1  |\n| AWS_MANAGER_GROUP_NAME | AWS group name for manager nodes. |\n| AWS_WORKER_GROUP_NAME | AWS group name for worker nodes.",
            "title": "AWS Node Scaling Envronment Variables"
        },
        {
            "location": "/configuration/#aws-secrets-file",
            "text": "AWS secret file defines the necessary environment variables to authenticate with AWS.  echo   'export AWS_ACCESS_KEY_ID=xxxx  export AWS_SECRET_ACCESS_KEY=xxxx  '   |  docker secret create aws -",
            "title": "AWS Secrets file"
        },
        {
            "location": "/feedback-and-contribution/",
            "text": "Feedback and Contribution\n\u00b6\n\n\nThe \nDocker Scaler\n project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are:\n\n\n\n\nCode patches or new features via pull requests\n\n\nDocumentation improvements\n\n\nBug reports and patch reviews\n\n\n\n\nReporting an Issue\n\u00b6\n\n\nFeel fee to \ncreate a new issue\n. Include as much detail as you can.\n\n\nIf an issue is a bug, please provide steps to reproduce it.\n\n\nIf an issue is a request for a new feature, please specify the use-case behind it.\n\n\nContributing To The Project\n\u00b6\n\n\nThis project is developed using \nTest Driven Development\n. When a new feature is added please run through the testing procedure:\n\n\nFork repo\n\u00b6\n\n\ngit clone https://github.com/thomasjpfan/docker-scaler\n\n\n\n\nUnit Testing\n\u00b6\n\n\nmake unit_test\n\n\n\n\nBuild\n\u00b6\n\n\nmake build\n\n\n\n\nTest\n\u00b6\n\n\nmake deploy_test\n\nmake integration_test\n\n\n\n\nCleanup\n\u00b6\n\n\nmake undeploy_test",
            "title": "Feedback and Contribution"
        },
        {
            "location": "/feedback-and-contribution/#feedback-and-contribution",
            "text": "The  Docker Scaler  project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are:   Code patches or new features via pull requests  Documentation improvements  Bug reports and patch reviews",
            "title": "Feedback and Contribution"
        },
        {
            "location": "/feedback-and-contribution/#reporting-an-issue",
            "text": "Feel fee to  create a new issue . Include as much detail as you can.  If an issue is a bug, please provide steps to reproduce it.  If an issue is a request for a new feature, please specify the use-case behind it.",
            "title": "Reporting an Issue"
        },
        {
            "location": "/feedback-and-contribution/#contributing-to-the-project",
            "text": "This project is developed using  Test Driven Development . When a new feature is added please run through the testing procedure:",
            "title": "Contributing To The Project"
        },
        {
            "location": "/feedback-and-contribution/#fork-repo",
            "text": "git clone https://github.com/thomasjpfan/docker-scaler",
            "title": "Fork repo"
        },
        {
            "location": "/feedback-and-contribution/#unit-testing",
            "text": "make unit_test",
            "title": "Unit Testing"
        },
        {
            "location": "/feedback-and-contribution/#build",
            "text": "make build",
            "title": "Build"
        },
        {
            "location": "/feedback-and-contribution/#test",
            "text": "make deploy_test\n\nmake integration_test",
            "title": "Test"
        },
        {
            "location": "/feedback-and-contribution/#cleanup",
            "text": "make undeploy_test",
            "title": "Cleanup"
        },
        {
            "location": "/license/",
            "text": "Docker Scaler License (MIT)\n\u00b6\n\n\nCopyright \u00a9 2017 Thomas Fan\n\n\nThe MIT License (MIT)\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "License"
        },
        {
            "location": "/license/#docker-scaler-license-mit",
            "text": "Copyright \u00a9 2017 Thomas Fan  The MIT License (MIT)  Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
            "title": "Docker Scaler License (MIT)"
        }
    ]
}